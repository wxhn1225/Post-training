# RL vs SFT

> 原帖:[AI basics April 24: RL vs SFT](https://x.com/NandoDF/status/1915351835105169534)

我在Twitter上开始了一个新的尝试：每日发布AI科普内容，希望提高信噪比。我将从大语言模型的强化学习(**RL for LLM**)开始，之后会拓展到扩散模型(**diffusion**)、流匹配(**flow matching**)等领域，看看这段旅程会带我们去向何方。言归正传，

## 4月24日：强化学习(**RL**) vs 监督微调(**SFT**)
监督学习(**Supervised learning**)本质上就是最朴素的模仿方式：照猫画虎。它采用最大似然估计（这也是我们预训练(**pretrain**)和监督微调(supervised-fine-tune, **SFT**)大语言模型(**LLM**)的方式）将世界状态（如文本问题）映射到行为（如文本回答）。我们称这类映射为策略(**policy**)。*Supervised learning*需要高质量的专家数据。学生仅是复刻教师的行为，因此教师本身必须足够优秀。更重要的是，教师只展示"怎么做"，但不进行评分。

此外，借助通用专家知识还存在一些非常强大的*Supervised learning*方式：下一步预测（联想学习）(**associative learning**)和重构学习(**re-construction**)。这基本上就是大语言模型预训练(**pretraining of LLM**)的工作方式，也是*diffusion*、*flow matching*和自编码器(**autoencoders**)在多模态感知与生成中的原理。人们常将这些技术称为无监督学习(**unsupervised**)。从某种意义上说，学习预测下一个比特本质上是一种自由能（熵）最小化过程，简而言之：在倾向于混乱的世界中创造秩序。这正是细胞和生命运作的方式，[Erwin_Schrödinger](https://en.wikipedia.org/wiki/Erwin_Schrödinger)和[Paul_Nurse](https://en.wikipedia.org/wiki/Paul_Nurse)分别撰写的两本[《What Is Life》](https://en.wikipedia.org/wiki/What_Is_Life?)对此有深入阐述。如果生命以此方式运作，那么智能也以类似方式运作并不奇怪。

而强化学习(Reinforcement learning, **RL**)则专注于选择性模仿，这对优化人们关心的特定任务性能非常有效。*RL*可以从智能体(**agent**)自身或其他*agent*先前产生的大量次优经验数据中学习。*RL*能够利用价值函数或其他工具（通过奖励学习获得）来识别和筛选有用信号。这种筛选过程使模型能够从海量低成本的次优数据中学习，并最终超越最优秀的教师。也就是说，在*RL*中，*agent*自行判断哪些数据对学习有价值，哪些数据应被忽略。我们不会模仿父母的所有行为，而是选择性地学习有益部分，有意识地遗忘不良示范——这正是心理治疗的核心理念！

*RL*关乎自我成长。*agent*能够生成数据，因此可以从自身数据（成功与失误）以及其他*agent*的混合数据中学习。当我们进一步利用奖励信号构建选择机制(**selection mechanism**)（例如对数据排序并仅选取最优的一半）时，*agent*便能开始从自身数据中学习并实现自我提升，这一过程具有强大的潜力。更重要的是，*agent*运用所获知识决定在环境(**environment**)中采取何种行动（测试、工具、实验），从而获取干预性因果知识。在[《An Invitation to Imitation》](https://kilthub.cmu.edu/ndownloader/files/12033137)一文中，Aurora Innovation首席科学家、卡内基梅隆大学教授Drew Bagnell探讨了一种名为"Dagger"的RL替代方案，其中*agent*采取行动而教师对其进行纠正。*Agent*从自身行动和经验中学习以获得稳健性至关重要。例如，若*agent*仅通过专家驾驶员产生的数据学习驾驶，某天一旦偏离道路（这是完美教师从未经历过的情况），学生将不知所措。为了学会回归正轨，此时学生需要教师的指导。[@AdaptiveAgents](https://x.com/AdaptiveAgents)在其[《delusions in sequence models for interaction and control》](https://arxiv.org/abs/2110.10819)中对此进行了深入阐述。

这里有个重要的研究启示：生成模型对*RL*的成功与任何RL算法创新同等重要。这可能有争议，但我认为过去10年RL的进步实际上源于生成模型的进步。从算法角度来看，正如我们将在这个系列中看到的，AI社区几乎都在使用已存在超过50年的简单算法思想，如期望最大化算法(**EM algorithm**)和策略梯度法(**policy gradient**)。然而，RL基础设施的规模已显著增长。

在 [《The Beginning of Infinity》](https://en.wikipedia.org/wiki/The_Beginning_of_Infinity)一书中，[@DavidDeutschOxf](https://x.com/DavidDeutschOxf)有力地论证了生成与选择的结合是智能的关键。

当然，Charles Darwin是这种思维方式的先驱。

读完这篇文章后，我希望你能感受到关于*unsupervised* vs *supervised* vs *RL*的最终定论尚未确定。此外，我不确定这些区分是否有用，但出于教学目的，我仍将在后续讨论中沿用这种分类方法。