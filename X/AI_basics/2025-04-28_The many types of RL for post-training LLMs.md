# The many types of RL for post-training LLMs

> 原贴: [The many types of RL for post-training LLMs](https://x.com/NandoDF/status/1916835195992277281)

RL有着多种形式和变体。正是这种多样性使得研究人员能在这个领域工作多年，并仍不时有新的惊喜。

![RL settings_img.jpg](../src/RL%20settings_img.jpg)

![RL settings_txt.jpg](../src/RL%20settings_txt.jpg)

首先，很多"使用RL微调LLMs"的例子实际上是单步RL问题(如上图左上)。在这类问题中，模型根据给定提示生成单一动作并获得评估。单一动作可以是文本回答、思维链(CoT)推理序列、草图、语音或任何其他行为信号，即任何标记序列。评估通常是单一结果奖励，例如回答是否正确。

在与聊天机器人的多步对话中，用户是环境，聊天机器人是智能体。当用户不提供输入时，决定下一步说什么是单步RL问题。从图表(左上)可以看出，三个动作可以合并为一个单一动作而不破坏决策图结构。然而，规划整个对话以实现最终目标，其中用户和聊天智能体都在适应，则是多步RL问题(图表左下)。这种设置也可模拟聊天机器人使用工具的情况，如网络浏览器、编译器或科学实验室来收集信息。当LLM策略选择工具(如实验室测试)时，它从测试中收集信息并输入到语言模型中，以决定下一步行动。

RL可能涉及多个步骤，每一步世界都在变化。因此，获得奖励时，我们不知道哪些决策导致了这个奖励。这被称为信用分配问题。

由于多步骤特性，RL具有组合性和高维度特点。有时视界是无限的，有时视界未知，即决策步骤数量不确定。在这些情况下，我们必须解决跨维度推理问题。

简言之，RL非常困难，解决方案的方差可能很高。研究人员发明了一系列概念来控制方差，代价是引入偏差，包括价值函数(表示预期奖励的函数，包含对未来奖励折扣的估计)。这些概念在多步决策问题中有用，但单步RL中不总是必需。虽然某些方法在电脑游戏中表现良好，但在LLMs中却失效了。

在控制领域，考虑具有T步骤、二次奖励函数和线性模型的决策问题很常见。这些被称为线性二次高斯控制器或调节器，构成了模型预测控制(MPC)这一广泛应用控制类型的基础。

然而，盲目将为电脑游戏或控制开发的RL方法理论和软件应用到语言模型领域是危险的。这些方法需处理游戏引擎的异步昂贵数据生成和陈旧的重放缓冲区。相反，重要的是测量系统中所有组件的效率，并针对可用硬件和交互类型优化算法。

对于工具使用和多步辅助，我们需要LLMs的多步RL。然而，要实现DeepSeek-R1或测试时强化学习(TTRL)等方法的功能，可能首先解决相对简单的单步RL问题就足够了。一些软件，如后文将介绍的策略梯度，对单步和多步RL可能非常相似，因此我们不妨使用通用灵活的软件框架。

所有RL智能体都能自学习和自我提升。设计得当时，它们可以构建质量不断提高的数据集，从而形成质量不断提高的策略。RL智能体的这一特性对性能和安全性都至关重要。在单步RL中，我们重复解决单步问题的过程，改进应随每次迭代增长。

还有一些更复杂的RL情况未在此讨论。有时决策视界未知或无限，增加了推理复杂性。此外，时间步可以是连续的或中断驱动的。动作和观察可以是离散和连续的混合，如带插图的文本文档，奖励可结合多个期望组件。还有合作和对抗环境下的多智能体扩展。

出于教学目的，我们明天将首先介绍最简单的情况：单步RL。我们将介绍主要算法，这足以解释deepseek-R1、语言建模的强化自训练(ReST)以及超越人类数据的扩展自训练(ReST^EM)等方法。之后，我们将讨论支持工具使用的多步设置。我们有意避免引入过多复杂性，因为更简单的方法通常足以用于LLM微调。