I'm starting a new thing here on twitter. Daily posts on AI educational material to improve the signal to noise ratio. I'll start with RL for LLMs, but move on to diffusion, flow matching and see where the journey takes us. Without further ado,
April 24: RL vs SFT
Supervised learning corresponds to the most trivial form of imitation: mimicking. It uses maximum likelihood (the way we pretrain and supervised-fine-tune (SFT) LLMs) to map states of the world (e.g. text questions) to actions (e.g. text answers). We call such mappings policies. Supervised learning requires good expert data. The student simply mimics the behaviour of the teacher, so the teacher has to be good. Moreover, the teacher demonstrates what to do, but does not grade.
As an aside, there exist very powerful ways of doing supervised learning with very general experts: next-step prediction (associative learning) and re-construction. This is basically how pretraining of LLMs works, or how diffusion, flow matching and autoencoders work in multimodal perception and generation. People often refer to these techniques as unsupervised. In a sense, learning to predict the next bit is a form of free energy (entropy) minimization, or in simple words: creating order in a world tending to disorder. That is how cells and life works, and there are two books titled What Is Life?, one by Erwin Schr√∂dinger and the other by Paul Nurse, that expand on this. If life works like this, it is not surprising that intelligence might also work like this.
Reinforcement learning (RL) on the other hand is about selective imitation, which is great for optimising performance at specific tasks that people care about. RL can be trained from tons of suboptimal experience data previously produced by the agent or other agents. RL can leverage value functions or other tools (learned with rewards) to identify and select useful signals. This process of selection empowers the models to learn with tons of cheap suboptimal data, and subsequently outperform the best teachers. That is, in RL the agent identifies what data is useful for learning and what data should be ignored. We don't imitate everything our parents do, but instead we choose what bits to imitate and which bits to rather forget -- therapy is great for this!
RL is about self-growth. The agents generate data. Hence the agents can learn from their own data (successes and mistakes), as well as a mix of data from other agents. When, in addition, we use reward signals to construct selection mechanisms (e.g. rank the data and pick only the best half), the agent can start learning from its own data and self-improve. This is incredibly powerful. Moreover, the agent uses its acquired knowledge to decide what actions (tests, tools, experiments) to conduct in the environment, thus resulting in interventional causal knowledge. In An Invitation to Imitation, Drew Bagnell, chief scientist at @aurora_inno  and prof at CMU, discusses an alternative to RL called Dagger, whereby the agent takes actions and the teacher corrects the student. It is important for the agent to learn from its actions and own experience so it learns to be robust. For example, if the agent learns to drive using data produced by an expert driver, and finds itself one day leaving the road (a situation that never happened to the perfect teacher), the student will not know what to do. For the student to learn to go back to the road, it needs teacher advice at that point. In delusions in sequence models for interaction and control @AdaptiveAgents expands on this.
An important research lesson here is that generative models are as important for RL to work as any RL algorithm innovations. This may be controversial, but I believe that progress in generative models is what has resulted in progress in RL over the last 10 years. Algorithmically, as we will see in this thread, almost everyone in the AI community is using simple algorithmic ideas that have existed for over 50 years, eg EM algorithm & policy gradients. However, the scale of the RL infrastructure has grown substantially.
In his book, The Beginning of Infinity @DavidDeutschOxf  makes a good case for generation accompanied by selection being the key to intelligence.
Charles Darwin of course pioneered this thinking. 
I hope that after reading this, you get the feeling that the final word on unsupervised vs supervised vs RL hasn't yet been written. Also, I'm not sure these distinctions are helpful, but I will use this categorisation nonetheless simply for teaching purposes in days to come.