# RL vs SFT

> 原帖: [April 24: RL vs SFT](https://x.com/NandoDF/status/1915351835105169534)

我在Twitter上开始了一个新的尝试：每日发布AI科普内容，希望提高信噪比。我将从大语言模型的强化学习(**RL for LLM**)开始，之后会拓展到扩散模型(**diffusion**)、流匹配(**flow matching**)等领域，看看这段旅程会带我们去向何方。言归正传，

## 4月24日：强化学习(**RL**) vs 监督微调(**SFT**)
监督学习**（Supervised Learning）**对应着最基础的模仿方式：复制。它采用最大似然估计（Maximum Likelihood）——即我们预训练和监督微调**（Supervised-Fine-Tune, SFT）**大语言模型**（LLM）**的方式——来建立从世界状态（如文本问题）到动作（如文本回答）的映射。我们称这类映射为策略**（Policy）**。Supervised learning依赖于优质的专家数据。学生只是简单地复制教师的行为，因此教师本身必须优秀。此外，教师只负责演示，而不负责评分。

顺带一提，还存在一些非常强大的方式，能利用非常通用的“专家”进行Supervised learning：下一步预测**（Next-Step Prediction，关联学习）**和重构**（Re-construction）**。这基本上就是pretraining of LLM的工作方式，也是扩散模型**（Diffusion）**、流匹配**（Flow Matching）**和自编码器**（Autoencoder）**在多模态感知与生成中的原理。人们常将这些技术称为无监督**（unsupervised）**。某种意义上，学习预测下一个比特是自由能（熵）最小化**（Free Energy Minimization）**的一种形式，简言之：在一个趋向无序的世界中创造秩序。这正是细胞和生命运作的方式，[Erwin_Schrödinger](https://en.wikipedia.org/wiki/Erwin_Schrödinger)和[Paul_Nurse](https://en.wikipedia.org/wiki/Paul_Nurse)的两本同名著作[《What Is Life》](https://en.wikipedia.org/wiki/What_Is_Life?)对此有深入阐述。如果生命如此运作，那么智能也可能遵循类似原理也就不足为奇了。

相比之下，强化学习**（Reinforcement learning, RL）**关乎选择性模仿**（Selective Imitation）**，这使其非常擅长优化人们关心的特定任务性能。RL可以从智能体**（agent）**自身或其他agent先前产生的大量次优经验数据中学习。RL能够利用价值函数（Value Functions）或其他工具（通过奖励学习获得）来识别**（Identify）**和筛选**筛选（Select）**有用信号。这种选择过程使模型能够从海量低成本的次优数据中学习，并最终超越最好的教师。也就是说，在RL中，agent能辨别哪些数据对学习有用，哪些应被忽略。我们并非模仿父母的所有行为，而是选择模仿哪些、遗忘哪些——这正是心理治疗的核心理念！

RL关乎自我成长**（Self-Growth）**。Agent能够生成数据，因此可以从自身数据（成功与错误）以及其他agent数据的混合中学习。当我们进一步利用奖励信号构建选择机制**（Selection Mechanism）**（例如对数据排序并仅选取最优的一半）时，agent便能开始从自身数据中学习并实现自我提升，这一过程具有强大的潜力。此外，agent运用所获知识决定在环境**(environment)**中采取何种行动（测试、工具使用、实验），从而获取干预性因果知识**（Interventional Causal Knowledge）**。在[《An Invitation to Imitation》](https://kilthub.cmu.edu/ndownloader/files/12033137)一文中，Aurora Innovation首席科学家、卡内基梅隆大学教授Drew Bagnell探讨了一种名为"Dagger"的RL替代方案：agent执行动作，而教师对其进行纠正。Agent从自身行动和经验中学习以获得稳健性至关重要。例如，若agent仅通过完美教师（专家司机）产生的数据学习驾驶，某天一旦偏离道路（教师从未遇到的情况）。要让学生学会返回道路，它需要教师在那时给予建议。[@AdaptiveAgents](https://x.com/AdaptiveAgents)在其[《delusions in sequence models for interaction and control》](https://arxiv.org/abs/2110.10819)中对此进行了深入阐述。

这里有个重要的研究启示：生成模型**（Generative Model）**对RL的成功的重要性不亚于任何RL算法创新。这或许有争议，但我认为，过去十年 RL 的进展很大程度上归功于Generative Model的进步。法层面，正如本系列推文将展示的，AI 社区几乎都在使用存在超过 50 年的基础思想，如期望最大化算法**（EM Algorithm）**和策略梯度**（policy gradient）**。然而，RL基础设施的规模已显著增长。

在 [《The Beginning of Infinity》](https://en.wikipedia.org/wiki/The_Beginning_of_Infinity)一书中，[@DavidDeutschOxf](https://x.com/DavidDeutschOxf)有力地论证了生成**（Generation）**与选择**（Selection）**的结合是智能的关键。

当然，Charles Darwin是这种思维方式的先驱。

读完这篇文章后，我希望你能感受到关于unsupervised vs supervised vs RL的讨论尚无定论。同时，我不确定这些区分是否有益，但在接下来的教学中，我仍将使用此分类法，仅出于教学目的。