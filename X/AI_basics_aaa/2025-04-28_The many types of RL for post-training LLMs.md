RL comes in many shapes and forms. It is this variety that makes people work on the problem for years and still be surprised now and then.
To start, many examples of ``finetuning LLMs with RL'' are one-step RL problems (top left of the figure above). Here, given a prompt the model generates a single action and gets an evaluation. A single action can be a text answer, Chain of Thought (CoT) reasoning sequence, sketch, speech, any other behaviour signal, i.e. any sequence of tokens. The evaluation is often a single outcome reward, e.g. whether the answer is correct.  
In a multi-step conversation with a chatbot, the user is the environment and the chatbot the agent. Deciding what to say next, while the user does not provide any inputs, is a one-step RL problem. This is clear in our figure (top left) because the three actions can be easily combined into a single one without destroying the structure of the decision graph. However, planning an entire conversation to achieve a goal at the end, during which both user and chat agent adapt, is a multi-step RL problem (our figure, bottom left). This setting could also model a chatbot using tools, such as a web browser, a compiler or a scientific lab, to gather information.  When the LLM policy selects a tool (e.g. a lab test) it gathers information from the test and this information is fed into the language model so that it can decide the next step of action.
RL can involve many steps, where at each step the world changes. As a result, when one gets a reward, one does not know which of the many decisions led to the reward. People call this the credit assignment problem. 
RL, because of the many-steps, is combinatorial and very high-dimensional. Sometimes the horizon is infinity, and sometimes we don't know the horizon, i.e. the number of decision steps is unknown. In these settings, we have to solve a trans-dimensional inference problem. 
In short, RL is really hard, and the variance of the solutions can be very high. Researchers have invented a series of concepts to keep the variance under check at the expense of introducing bias, including value functions (functions that represent the expected rewards, with some guess as to how to discount future rewards). These concepts are useful in multi-step decision problems, but not always needed for one-step RL. While some of these ideas work well for computer games, they fail with LLMs.
In control, it is very popular to consider decision problems with T steps, quadratic reward functions and linear models. These are known as linear quadratic Gaussian controllers or regulators, and form the basis of one of the most ubiquitous type of control: Model Predictive Control (MPC).
It is however dangerous to import blindly the theory and software of RL methods developed for computer games or control to the world of language models. Those methods had to deal with asynchronous expensive data generation with game engines and stale replay buffers. Instead, it is important to measure the efficiency of all components in the system and optimise the algorithms for the available hardware and types of interaction.
For tool-use and multi-step assistance, we need multi-step RL for LLMs. However, to do what methods like DeepSeek-R1  or Test-Time Reinforcement Learning - TTRL do, it may suffice to first solve the one-step RL problem, which is slightly simpler. Some of the software, e.g. for policy gradients introduced later in this document, might be very similar for both one-step and multi-step RL, so we may as well try to use a common flexible software framework.
All RL agents self-learn and self-improve. If designed well, they can construct datasets of increasing quality, which in turn lead to policies of increasing quality. This property of RL agents is essential for both performance and for safety. Note that in one-step RL, we repeat the process of solving one-step problems, and the improvements should grow with each iteration. 
There can be harder RL cases not addressed here. Sometimes the decision horizon is unknown or infinite, thus increasing the complexity of inference. Moreover, the time steps can be continuous or interrupt-driven. The actions and observations can be a mix of discrete and continuous, e.g. text documents with illustrations, and the reward can combine multiple desirable components. There are also extensions to multi-agents in both collaborative and adversarial settings. 
For didactic reasons, we will first cover the simplest case tomorrow: one-step RL. We will cover the main algorithms, and this will be enough to explain methods such as deepseek-R1, Reinforced Self-Training (ReST) for Language Modelling and Beyond Human Data: Scaling Self-Training (ReST^EM). Later, we will address multi-step settings, which allow for tool-use. We will avoid introducing more complexity purposely because the simpler methods are often enough for LLM finetuning.