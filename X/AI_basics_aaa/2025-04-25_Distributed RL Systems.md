An agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through reinforcement learning or teaching. Agents can have internal goals (inferred subgoals, and the desire to observe more, learn more, or control more - yes, we should be thinking safety here!) or external goals either specified as reward functions or as feedback reward signals. The following figure shows the main RL ingredients. 
An agent could be a multimodal neural net that interacts with a user (its environment) to empower the user with personalised education (the goal). The more the agent observes, the easier it is for the agent to create a personalised curriculum to assist the user. 
RL with industrial LLMs can involve millions of simultaneous interactions using billion parameter models, and entire data centres. It ain't cheap! It is far from trivial how to construct RL systems that work efficiently at such massive scales. Here, I simply provide a very shallow overview of such scalable distributed systems. Tip of the hat to all those amazing engineers at Anthropic, DeepMind, DeepSeek, Meta, Microsoft AI, OpenAI, X and so on! For me, they are as unique and exceptionally talented as the best football players in the Premier League. 
As discussed by some of my colleagues, see e.g.  IMPALA: Scalable Distributed Deep-RL and acme: A library of reinforcement learning, modern distributed RL systems can be broken down into two components: Actors and Learners. Each actor interacts with the environment by generating actions with a network known as the policy. The actors also gather rewards and observations from the environment. The gathered data is added to a common replay memory. The learner samples data from the replay memory and uses it to update the policy network. After updating the network, the weight checkpoints need to be sent to each actor. It is important when designing such systems to measure the duration of each operation, measure the bandwidth of each communication link, and so on. This demands precise engineering and comprehensive measurements and ablations. 
In language, the actors are chatbot agents and the environments are people. The data from each chat is then sent to a replay memory for learning. Typically, the learner may require more storage and compute than the actors because the learner needs to keep track of gradients and other massive scale statistics. 
It is important to know the costs of inference of the actors, the costs of communication and the costs of learning. In some settings, these costs allow for the agent to learn on-policy. This process is often asynchronous because different actors can gather data at different speeds and times.
If the data is not gathered fast enough, it may be necessary for the learner to replay old examples from memory to update the policy. This is the off-policy setting. Here, it is important to correct for the fact that the model is being learned with stale data - remember the example of driving in the April 24th Tweet? Being too off-policy can be dangerous! Fortunately, we will see later that researchers already have some solutions for this, for example importance weights and other weighting mechanisms, e.g., the weights that appear in Proximal policy optimization (PPO) and the DeepSeek-R1 paper.
Finally, sometimes it is possible to learn the policy from a big replay database alone. This is known as off-line RL or batch RL. Off-line RL is better than supervised learning because it includes selection mechanisms as discussed in the previous post, but it is of course not as good as on-line RL because it lacks direct generation of actions on environments. However, off-line RL can be really useful because it allows for learning in situations where interaction is either too costly or dangerous.
BTW, I'm happy to clarify any questions you might have. You may also completely disagree with me, even on dry technical topics, but all opinions are welcome. Like I need to say this on X ðŸ˜‚